---
title: "Prática 6 R"
João Pedro Moro Bolognini
Usando árvores de regressão e classificação, Naive Bayes, Support Vector Machines, k-Nearest Neighbors e K-means
---

Árvore de regressão - instalando e importando bibliotecas
```{r}
#install.packages("rpart")
#install.packages("forecast")
library(rpart)
library(forecast)
```

Visualizando a base de dados
```{r}
iris
```

Criando o modelo para prever o atributo Sepal.Length
```{r}
modelo = rpart(Sepal.Length ~ Sepal.Width + Petal.Length +  Species ,data=iris)
modelo
summary(modelo)
```

Visualização do modelo
```{r}
plot(modelo)
text(modelo)
```

Fazendo a predição do modelo
```{r}
predicao = predict(modelo, iris)
head(predicao)
```

Calculando a predição com os dados reais
```{r}
comparacao = cbind(predicao,iris$Sepal.Length,predicao - iris$Sepal.Length )
head(comparacao)
```

Visualizando as métricas obtidas do modelo
```{r}
accuracy(predicao,iris$Sepal.Length)
```

Árvore de Classificação - instalando e importando bibliotecas
```{r}
#install.packages('rpart',dependencies=T)
library(rpart)
```

Carregando os dados e separando-os em conjuntos de treino e teste
```{r}
credito = read.csv("Credit.csv")
credito
amostra = sample(2,1000,replace=T, prob=c(0.7,0.3))
creditotreino = credito[amostra==1,]
creditoteste = credito[amostra==2,]
```

Criando o modelo para o método de classificação a partir da classe
```{r}
arvore = rpart(class ~ ., data=creditotreino,  method="class")
print(arvore)
```

Imprimindo a arvore gerada
```{r}
plot(arvore)
text(arvore, use.n=TRUE, all=TRUE, cex=.8)
```

Fazendo a previsão do modelo
```{r}
teste = predict(arvore,newdata=creditoteste)
head(teste)
```

Adicionando as colunas teste e creditoteste
```{r}
cred = cbind(creditoteste,teste)
cred
```

Criando a coluna com o resultado categórico
```{r}
cred['Result'] = ifelse(cred$bad>=0.5,"bad","good")
cred
```

Criando a matriz de confusão
```{r}
confusao = table(cred$class,cred$Result)
confusao
taxaacerto = (confusao[1] + confusao[4]) / sum(confusao)
taxaacerto
```

Naive Bayes - instalando e importando bibliotecas
```{r}
#install.packages("e1071")
library(e1071)
```

Importando a base de dados
```{r}
credito = read.csv("Credit.csv")
head(credito)
dim(credito)
```

Transformando a classe em fator
```{r}
credito$class = as.factor(credito$class)
```

Separando os dados em conjuntos de treino e teste
```{r}
amostra = sample(2,1000,replace=T, prob=c(0.7,0.3))
amostra
creditotreino = credito[amostra==1,]
creditoteste = credito[amostra==2,]
```

Verificando as dimensões dos conjuntos
```{r}
dim(creditotreino)
dim(creditoteste)
```

Criando o modelo
```{r}
modelo <- naiveBayes(class ~., creditotreino)
modelo
```

Calculando a previsão do modelo
```{r}
predicao <- predict(modelo,creditoteste)
predicao
```

Verificando a matriz de Confusão
```{r}
confusao = table(creditoteste$class,predicao)
confusao
taxaacerto = (confusao[1] + confusao[4]) / sum(confusao)
taxaerro = (confusao[2] + confusao[3]) / sum(confusao)
taxaacerto
taxaerro
```

Support Vector Machines - instalando e importando bibliotecas
```{r}
#install.packages("e1071")
#install.packages("randomForest")
library(e1071)
library(randomForest)
```

Importando os dados e atribuindo a classe como fator
```{r}
credito = read.csv("Credit.csv")
credito$class = as.factor(credito$class)
credito
```


Dividindo os dados em conjuntos de treino e teste e definindo a semente para ser possível repetir o experimento
```{r}
set.seed(234)
amostra = sample(2,1000,replace=T, prob=c(0.7,0.3))
creditotreino = credito[amostra==1,]
creditoteste = credito[amostra==2,]
```

Criando um primeiro modelo com todos os atributos e avaliamos suaa acuracia
```{r}
modelo = svm(class ~., creditotreino)
predicao = predict(modelo,creditoteste)
confusao = table(creditoteste$class,predicao)
taxaacerto = (confusao[1] + confusao[4]) / sum(confusao)
taxaacerto
```
Aplicando um método de seleção de atributos
```{r}
importancia  = randomForest(class ~ ., data = creditotreino)
col = importance(importancia)
col
varImpPlot(importancia)
```

Criando um segundo modelo com as variáveis independentes mais importantes do conjunto de dados
```{r}
modelo = svm(class ~ credit_amount + age + duration + checking_status ,creditotreino)
predicao = predict(modelo,creditoteste)
confusao = table(creditoteste$class,predicao)
taxaacerto = (confusao[1] + confusao[4]) / sum(confusao)
taxaacerto
```

K-Nearest Neighbors - instalando e importando bibliotecas
```{r}
#install.packages("class")
library(class)
```

Visualizando o conjunto de dados Iris e o sumário dele
```{r}
head(iris)
summary(iris)
```

Dividindo os dados em conjuntos de treino e teste
```{r}
amostra = sample(2,150,replace=T, prob=c(0.7,0.3))
iristreino = iris[amostra==1,]
iristeste = iris[amostra==2,]
```

Visualizando as dimensões desses conjuntos
```{r}
dim(iristreino)
dim(iristeste)
```

Fazendo a classificação sem criar modelo com os dados de treino (instâncias), dados de teste, classe real, número de vizinhos considerados
```{r}
previsao = knn(iristreino[,1:4],iristeste[,1:4],iristreino[,5],k=3)
previsao
```
Visualizando a matriz de confusão
```{r}
tabela = table(iristeste[,5],previsao)
tabela
```

Avaliando a performance do modelo
```{r}
(tabela[1] + tabela[5] + tabela[9]) / sum(tabela) 
```

K-means - instalando e importando bibliotecas
```{r}
#install.packages("factoextra")
library(factoextra)

```

Visualizando o conjunto de dados iris
```{r}
iris
```

Gerando o cluster
```{r}
cluster = kmeans(iris[1:4],centers=3)
cluster
table(iris$Species,cluster$cluster)
```

Visualizando o gráfico
```{r}
plot(iris$Sepal.Length, iris$Sepal.Width,col=cluster$cluster, pch=20, main="iris")
plot(iris[,1:4],col=cluster$cluster, main="Iris")
```

Visualizando o gráfico de cada cluster separado
```{r}
g2 = fviz_cluster(cluster, data=iris[1:4], ellipse.type = "convex", ggtheme = theme_bw(),
                main="Cluster ")
plot(g2)
```

Adicionado o cluster aos dados originais
```{r}
iris2 = iris
iris2['grupos'] = cluster$cluster
iris2
write.csv(iris2,"iri2.csv")
```